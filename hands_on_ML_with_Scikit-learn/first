import os
import tarfile
from six.moves import urllib
import pandas as pd
from IPython.display import display
from tabulate import tabulate
import matplotlib.pyplot as plt
import numpy as np
import hashlib
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedShuffleSplit
from pandas.plotting import scatter_matrix

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml/master/"
HOUSING_PATH = "datasets/housing"
HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + "/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    if not os.path.isdir(housing_path):
        os.makedirs(housing_path)
    tgz_path = os.path.join(housing_path, "housing.tgz") 
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)

def color_negative_red(val):
    """
    Takes a scalar and returns a string with
    the css property `'color: red'` for negative
    strings, black otherwise.
    """
    color = 'blue' if val > 90 else 'black'
    return 'color: % s' % color

def split_train_test(data, test_ratio):
    # np.random.seed(42)
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data)*test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]

def test_set_check(identifier, test_ration, hash):
    return hash(np.int64(identifier)).digest()[-1] < 256*test_ration

def split_train_test_by_id(data, test_ratio, id_column, hash=hashlib.md5):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio, hash))
    return data.loc[~in_test_set], data.loc[in_test_set]

if __name__ == "__main__":
    #fetch_housing_data()
    housing = load_housing_data()
    df = housing.head()

    #show data in table using tabulate()
    print(tabulate(df, headers = 'keys', tablefmt = 'psql'))

    #The info() method is useful to get a quick description of the data
    housing.info()
    print()

    # #You can find out what categories exist and how many districts 
    # # belong to each category by using the value_counts() method
    # print(housing["ocean_proximity"].value_counts())
    # print()

    # #The describe() method shows a summary of the numerical attributes
    # #The 25%, 50%, and 75% rows show the corresponding percentiles: a percentile 
    # #indicates the value below which a given percentage of observations in a group of 
    # #observations falls. For example, 25% of the districts have a housing_median_age lower than
    # #while 50% are lower than 29 and 75% are lower than 37
    # print(housing.describe())
    # print()
    
    #A histogram shows the number of instances (on the vertical axis) 
    # that have a given value range (on the horizontal axis).
    # housing.hist(bins=50, figsize=(20,15))
    # plt.show()

    # When you estimate the generalization error using the test
    # set, your estimate will be too optimistic and you will launch a system that will not
    # perform as well as expected. This is called data snooping bias.
    # Creating a test set is theoretically quite simple: just pick some instances randomly,
    # typically 20% of the dataset, and set them aside:
    train_set, test_set = split_train_test(housing, 0.2)
    print(len(train_set), "train + ", len(test_set), "test")
    print()


    housing_with_id = housing.reset_index() # adds an 'index' column
    # train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, 'index')
    # print(housing_with_id['index'])

    housing_with_id['id'] = housing['longitude']*1000 +housing['latitude']
    # train_set, test_set = split_train_test_by_id(housing_with_id, 0,2, 'id')
    # print(housing_with_id['id'])


    train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
    # we have considered purely random sampling methods. This is generally fine if
    # your dataset is large enough (especially relative to the number of attributes), but if it
    # is not, you run the risk of introducing a significant sampling bias.



    # When a survey company decides to call 1,000 people to ask them a few questions, they don’t just pick
    # 1,000 people randomly in a phone booth. They try to ensure that these 1,000 people
    # are representative of the whole population. For example, the US population is com‐
    # posed of 51.3% female and 48.7% male, so a well-conducted survey in the US would
    # try to maintain this ratio in the sample: 513 female and 487 male. This is called strati‐
    # fied sampling: the population is divided into homogeneous subgroups called strata,
    # and the right number of instances is sampled from each stratum to guarantee that the
    # test set is representative of the overall population.



    #  median income is a very important attribute to predict median housing prices
    # to ensure that the test set is representative of the various categories of incomes in the whole dataset

    # important to have a sufficient number of instances in your dataset for each stratum

    # The following code creates an income category
    # attribute by dividing the median income by 1.5 (to limit the number of income cate‐
    # gories), and rounding up using ceil (to have discrete categories), and then merging
    # all the categories greater than 5 into category 5:
    housing['income_cat'] = np.ceil(housing['median_income']/1.5)
    housing['income_cat'].where(housing['income_cat'] < 5, 5, inplace=True)
    # print(housing['income_cat'])

    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
    for train_index, test_index in split.split(housing, housing['income_cat']):
        strat_train_set = housing.loc[train_index]
        strat_test_set = housing.loc[test_index]

    # income category proportions in the full housing dataset:
    # print(strat_test_set['income_cat'].value_counts()/len(strat_test_set))
    # print(housing['income_cat'].value_counts()/len(housing))

    # remove the income_cat attribute so the data is back to its original state
    # for set in(strat_train_set, strat_test_set):
    #   set.drop(['income_cat'], axis=1, inplace=True)


    # Discover and Visualize the Data to Gain Insights

    # create a copy so you can play with it without harming the training set
    housing = strat_train_set.copy()
    
    # pandas plots documentation https://pandas.pydata.org/pandas-docs/version/0.15.0/visualization.html
    # housing.plot(kind='scatter', x='longitude', y='latitude')
    
    # Setting the alpha option to 0.1 makes it much easier to visualize the places
    # where there is a high density of data points 
    # housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)

    
    # The radius of each circle represents the district’s population (option s), 
    # and the color represents the price (option c). We
    # will use a predefined color map (option cmap) called jet, which ranges from blue
    # (low values) to red (high prices)
    # housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4,
    # s=housing['population']/100, label='population',
    # c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True)
    # plt.legend()
    # plt.show()




    # # --------------------------------------------------
    # # Looking for Correlations
    # # --------------------------------------------------

    # # dataset is not too large, you can easily compute the standard correlation
    # # coefficient (also called Pearson’s r) between every pair of attributes using the corr() method
    # corr_matrix = housing.corr()
    # # print(corr_matrix)
    # print(corr_matrix['total_rooms'].sort_values(ascending=False))
    # # The correlation coefficient ranges from –1 to 1. When it is close to 1, it means that
    # # there is a strong positive correlation; for example, the median house value tends 
    # # to go up when the median income goes up.
    # # When the coefficient is close to –1, it means that there is a strong negative correlation
    # # Finally, coefficients close to zero mean that there is no linear correlation


    # # Another way to check for correlation between attributes is to use Pandas’
    # # scatter_matrix function
    
    # attributes = ["median_house_value", "median_income", "total_rooms",
    # "housing_median_age"]
    # scatter_matrix(housing[attributes], figsize=(12, 8))
    # plt.show()

    # # The most promising attribute to predict the median house value is the median
    # # income, so let’s zoom in on their correlation scatterplot 
    # housing.plot(kind="scatter", x="median_income", y="median_house_value", alpha=0.1)
    # plt.show()




    # # --------------------------------------------------
    # # Experimenting with Attribute Combinations
    # # --------------------------------------------------

    # to try out various attribute combinations.
    housing['rooms_per_household'] = housing['total_rooms']/housing['households']
    housing['bedrooms_per_room'] = housing['total_bedrooms']/housing['total_rooms']
    housing['population_per_household'] = housing['population']/housing['households']

    rooms_per_household = housing.values[:, 3]/housing.values[:, 6]
    print(rooms_per_household)

    # corr_matrix = housing.corr()
    # print('median_house_value')
    # print(corr_matrix['median_income'].sort_values())

    # attributes = ["median_house_value", "median_income", "population_per_household",
    # "bedrooms_per_room", 'rooms_per_household']
    # scatter_matrix(housing[attributes], figsize=(12, 8))
    # plt.show()



    # --------------------------------------------------
    # --------------------------------------------------
    # PREPARING THE DATA FOR MACHINE LEARNING ALGORITHMS
    # --------------------------------------------------
    # --------------------------------------------------

    # separate the predictors and the labels
    housing = strat_train_set.drop('median_house_value', axis=1)
    housing_labels = strat_train_set['median_house_value'].copy()




    # --------------------------------------------------
    # Data Cleaning, Imputer
    # --------------------------------------------------

    # total_bedrooms attribute has some missing values, so let’s fix this

    # Option 1: Get rid of the corresponding districts
    housing.dropna(subset='total_bedrooms')

    # Option 2: Get rid of the whole attribute
    housing.drop("total_bedrooms", axis=1)

    # Option 3: Set the values to some value (zero, the mean, the median...)
    median = housing['total_bedrooms'].median
    housing['total_bedrooms'].fillna(median)
    # don’t forget to save the median value, it was computed on training set
    # need it later to replace missing values in the test set for evaluation your system
   
    # Scikit-Learn provides a handy class to take care of missing values: Imputer
    from sklearn.impute import SimpleImputer 
    imputer = SimpleImputer(missing_values=np.nan, strategy='median')

    # median can only be computed on numerical attributes, we need to create a
    # copy of the data without the text attribute ocean_proximity: 
    housing_num = housing.drop("ocean_proximity", axis=1)

    # fit the imputer instance to the training data using the fit() method
    imputer.fit(housing_num)
    # The imputer has simply computed the median of each attribute and stored the result
    # in its statistics_ instance variable
    print(imputer.statistics_)
    print(housing_num.median().values)

    # use this “trained” imputer to transform the training set by replacing
    # missing values by the learned medians:
    X = imputer.transform(housing_num)
    # result is a plain Numpy array containing the transformed features.

    # back into a Pandas DataFrame
    housing_tr = pd.DataFrame(X, columns=housing_num.columns)
    housing_tr.info()
    print(housing_tr.describe())

    # Scikit-Learn Design
    # Consistency
    # — Estimators
    # — Transformers
    # — Predictors
    # Inspection
    #   All the estimator’s hyperparameters are accessible directly via public
    #   instance variables (e.g., imputer.strategy), and all the estimator’s learned
    #   parameters are also accessible via public instance variables with an underscore
    #   suffix (e.g., imputer.statistics_)
    # Nonproliferation of classes.
    # Composition
    # Sensible defaults



    # --------------------------------------------------
    # Handling Text and Categorical Attributes
    # --------------------------------------------------

    # from sklearn.preprocessing import LabelEncoder
    # encoder = LabelEncoder()
    # housing_cat = housing['ocean_proximity']
    # housing_cat_encoded = encoder.fit_transform(housing_cat)
    # print(housing_cat_encoded)
    # print(encoder.classes_)
    # print(housing['ocean_proximity'].value_counts())
    # print(housing['ocean_proximity'].values)
    # # issue with this representation is that ML algorithms will assume that two nearby
    # # values are more similar than two distant values.
    # # To solve this problem is used one-hot encoding (only one attribute will be equal to 1 (hot), while the
    # # others will be 0 (cold)).
    
    # from sklearn.preprocessing import OneHotEncoder
    # encoder = OneHotEncoder()
    # # fit_transform() expects a 2D array, but housing_cat_encoded is a 1D array, so we
    # # need to reshape it
    # housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))
    # # Notice that the output is a SciPy sparse matrix, instead of a NumPy array
    # print(housing_cat_1hot.toarray())

    # # substitute both transformations (from text categories to integer categories, then
    # # from integer categories to one-hot vectors) in one shot using the LabelBinarizer class
    # from sklearn.preprocessing import LabelBinarizer
    # encoder = LabelBinarizer()
    # housing_cat_1hot = encoder.fit_transform(housing_cat)
    # print(housing_cat_1hot)
    # # this returns a dense NumPy array by default. You can get a sparse matrix
    # # instead by passing sparse_output=True to the LabelBinarizer constructor.



    # --------------------------------------------------
    # Custom Transformers
    # --------------------------------------------------

    # created class, implement three methods: fit() (returning self), transform(), and fit_transform()
    # get the last one for free by simply adding TransformerMixin as a base class. Also, if add BaseEstimator 
    # as a base class (and avoid *args and **kargs in your constructor)  will get
    # two extra methods (get_params() and set_params()) that will be useful for auto‐
    # matic hyperparameter tuning.
    from sklearn.base import BaseEstimator, TransformerMixin

    rooms_ix, bedrooms_ix, population_ix, household_ix = 3,4,5,6

    class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
        def __init__(self, add_bedrooms_per_room = True): #no args, no kargs
            self.add_bedrooms_per_room = add_bedrooms_per_room

        def fit(self, X, y=None):
            return self # nothing else to do

        def transform(self, X, y=None):
            rooms_per_household = X[:, rooms_ix]/X[:, household_ix]
            population_per_household = X[:, population_ix]/X[:, household_ix]
            if self.add_bedrooms_per_room :
                bedrooms_per_room= X[:, bedrooms_ix]/X[:, rooms_ix]
                return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]
            else:
                return np.c_[X, rooms_per_household, population_per_household]

    attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)
    housing_extra_attribs = attr_adder.transform(housing.values)



    # # --------------------------------------------------
    # # Feature Scaling
    # # --------------------------------------------------

    # # There are two common ways to get all attributes to have the same scale: min-max
    # # scaling and standardization.

    # # Min-max scaling (many people call this normalization) is quite simple: values are
    # # shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract‐
    # # ing the min value and dividing by the max minus the min. Scikit-Learn provides a
    # # transformer called MinMaxScaler for this. It has a feature_range hyperparameter
    # # that lets you change the range if you don’t want 0–1 for some reason.

    # # Standardization is quite different: first it subtracts the mean value (so standardized
    # # values always have a zero mean), and then it divides by the variance so that the result‐
    # # ing distribution has unit variance
    # # Scikit-Learn provides a transformer called StandardScaler for standardization.
    # # X_new = (X - mean)/Std



    # --------------------------------------------------
    # Transformation Pipelines
    # --------------------------------------------------

    # The Pipeline constructor takes a list of name/estimator pairs defining a sequence of
    # steps. All but the last estimator must be transformers (i.e., they must have a
    # fit_transform() method)

    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler, LabelBinarizer
    from sklearn.impute import SimpleImputer 

    # num_pipeline = Pipeline([
    #     ('imputer', Imputer(strategy='median')),
    #     ('attribs_adder', Combined AttributesAdder()),
    #     ('std_scaler', StandardScaler()),])

    num_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy="median")),
        ('attribs_adder', CombinedAttributesAdder()),
        ('std_scaler', StandardScaler()),
    ])
    housing_num_tr = num_pipeline.fit_transform(housing_num)

    # new class
    class NewLabelBinarizer(LabelBinarizer):
        def fit(self, X, y=None):
            return super(NewLabelBinarizer, self).fit(X)
        def transform(self, X, y=None):
            return super(NewLabelBinarizer, self).transform(X)
        def fit_transform(self, X, y=None):
            return super(NewLabelBinarizer, self).fit(X).transform(X)

    # simple custom transformer to handle Pandas DataFrames
    from sklearn.base import BaseEstimator, TransformerMixin

    class DataFrameSelector(BaseEstimator, TransformerMixin):
        def __init__(self, attribute_names):
            self.attribute_names = attribute_names
        def fit(self, X, y=None):
            return self
        def transform(self, X):
            return X[self.attribute_names].values

    # FeatureUnion class join transformations into a single pipeline
    from sklearn.pipeline import FeatureUnion

    num_attribs = list(housing_num)
    cat_attribs = ['ocean_proximity']

    print('Housing num')
    print(housing_num)
    print(list(housing_num))

    num_pipeline = Pipeline([
        ('selector', DataFrameSelector(num_attribs)),
        ('imputer',SimpleImputer(strategy='median')),
        ('attribs_adder', CombinedAttributesAdder()),
        ('std_scaler', StandardScaler()),
    ])

    cat_pipeline = Pipeline([
            ('selector', DataFrameSelector(cat_attribs)),
            ('label_binarizer', NewLabelBinarizer()),        
    ])

    full_pipeline = FeatureUnion(transformer_list=[
            ('num_pipeline', num_pipeline),
            ('cat_pipeline', cat_pipeline), 
    ])

    housing_prepared = full_pipeline.fit_transform(housing)
    # print(housing_prepared)


    # --------------------------------------------------
    # --------------------------------------------------
    # PREPARING THE DATA FOR MACHINE LEARNING ALGORITHMS
    # --------------------------------------------------
    # --------------------------------------------------

    # --------------------------------------------------
    # Training and Evaluating on the Training Set
    # --------------------------------------------------

    from sklearn.linear_model import LinearRegression

    lin_reg = LinearRegression()
    lin_reg.fit(housing_prepared, housing_labels)

    some_data = housing.iloc[:5]
    some_labels = housing_labels.iloc[:5]
    some_data_prepared = full_pipeline.transform(some_data)
    print("Predictions:\t", lin_reg.predict(some_data_prepared))
    print('Labels:\t\t', list(some_labels))

    from sklearn.metrics import mean_squared_error
    housing_predictions = lin_reg.predict(housing_prepared)
    lin_mse = mean_squared_error(housing_labels, housing_predictions)
    lin_rmse = np.sqrt(lin_mse)
    print('Lin_RMSE: ',lin_rmse)
    # Typical prediction error of $68,628 is not very satisfying. 
    # This is an example of a model underfitting the training data. 
    # When this happens it can mean that the features do not provide
    # enough information to make good predictions, or that the model is not powerful enough.
    # the main ways to fix underfitting are to select a more powerful model, 
    # to feed the training algorithm with better features, or
    # to reduce the constraints on the model.

    # DecisionTreeRegressor. This is a powerful model, capable of finding
    # complex nonlinear relationships in the data
    from sklearn.tree import DecisionTreeRegressor

    tree_reg = DecisionTreeRegressor()
    tree_reg.fit(housing_prepared, housing_labels)
    housing_predictions = tree_reg.predict(housing_prepared)
    tree_mse = mean_squared_error(housing_labels, housing_predictions)
    tree_rmse = np.sqrt(tree_mse)
    # tree_rmse = 0, it means the model has badly overfit the data
    # we don’t want to touch the test set until we are ready to launch a
    # model we are confident about, so we need to use part of the training 
    # set for training, and part for model validation.


    # --------------------------------------------------
    # Better Evaluation Using Cross-Validation
    # --------------------------------------------------

    # A great alternative is to use Scikit-Learn’s cross-validation feature. The following code
    # performs K-fold cross-validation: it randomly splits the training set into 10 distinct
    # subsets called folds, then it trains and evaluates the Decision Tree model 10 times,
    # picking a different fold for evaluation every time and training on the other 9 folds.
    # The result is an array containing the 10 evaluation scores:
    from sklearn.model_selection import cross_val_score

    scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)
    tree_rmse_scores = np.sqrt(-scores)

    def display_scores(scores):
        print("Scores:", scores)
        print("Mean:", scores.mean())
        print("Standard deviation:", scores.std())

    # display_scores(tree_rmse_scores)

    lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,
    scoring="neg_mean_squared_error", cv=10)
    lin_rmse_scores = np.sqrt(-lin_scores)

    # display_scores(lin_rmse_scores)
    # the Decision Tree model performs worse than the Linear Regression model

    from sklearn.ensemble import RandomForestRegressor

    forest_reg = RandomForestRegressor()
    forest_reg.fit(housing_prepared, housing_labels)
    housing_predictions = forest_reg.predict(housing_prepared)
    forest_mse = mean_squared_error(housing_labels, housing_predictions)
    forest_rmse = np.sqrt(tree_mse)

    forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)
    forest_rmse_scores = np.sqrt(-forest_scores)

    display_scores(forest_rmse_scores)



    







