import os
import tarfile
from six.moves import urllib
import pandas as pd
from IPython.display import display
from tabulate import tabulate
import matplotlib.pyplot as plt
import numpy as np
import hashlib
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedShuffleSplit

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml/master/"
HOUSING_PATH = "datasets/housing"
HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + "/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    if not os.path.isdir(housing_path):
        os.makedirs(housing_path)
    tgz_path = os.path.join(housing_path, "housing.tgz") 
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)

def color_negative_red(val):
    """
    Takes a scalar and returns a string with
    the css property `'color: red'` for negative
    strings, black otherwise.
    """
    color = 'blue' if val > 90 else 'black'
    return 'color: % s' % color

def split_train_test(data, test_ratio):
    # np.random.seed(42)
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data)*test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]

def test_set_check(identifier, test_ration, hash):
    return hash(np.int64(identifier)).digest()[-1] < 256*test_ration

def split_train_test_by_id(data, test_ratio, id_column, hash=hashlib.md5):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio, hash))
    return data.loc[~in_test_set], data.loc[in_test_set]

if __name__ == "__main__":
    #fetch_housing_data()
    housing = load_housing_data()
    df = housing.head()

    #show data in table using tabulate()
    print(tabulate(df, headers = 'keys', tablefmt = 'psql'))

    #The info() method is useful to get a quick description of the data
    housing.info()
    print()

    #You can find out what categories exist and how many districts 
    # belong to each category by using the value_counts() method
    print(housing["ocean_proximity"].value_counts())
    print()

    #The describe() method shows a summary of the numerical attributes
    #The 25%, 50%, and 75% rows show the corresponding percentiles: a percentile 
    #indicates the value below which a given percentage of observations in a group of 
    #observations falls. For example, 25% of the districts have a housing_median_age lower than
    #while 50% are lower than 29 and 75% are lower than 37
    print(housing.describe())
    print()
    
    #A histogram shows the number of instances (on the vertical axis) 
    # that have a given value range (on the horizontal axis).
    housing.hist(bins=50, figsize=(20,15))
    # plt.show()

    # When you estimate the generalization error using the test
    # set, your estimate will be too optimistic and you will launch a system that will not
    # perform as well as expected. This is called data snooping bias.
    # Creating a test set is theoretically quite simple: just pick some instances randomly,
    # typically 20% of the dataset, and set them aside:
    train_set, test_set = split_train_test(housing, 0.2)
    print(len(train_set), "train + ", len(test_set), "test")
    print()


    housing_with_id = housing.reset_index() # adds an 'index' column
    # train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, 'index')
    # print(housing_with_id['index'])

    housing_with_id['id'] = housing['longitude']*1000 +housing['latitude']
    # train_set, test_set = split_train_test_by_id(housing_with_id, 0,2, 'id')
    # print(housing_with_id['id'])

    # print("Hash_key")
    # disc = hashlib.md5(np.int64(housing_with_id['index'][56])).digest()
    # len_ = len(hashlib.md5(np.int64(housing_with_id['index'][56])).digest())
    # key1 = hashlib.md5(np.int64(housing_with_id['index'][56])).digest()[10]

    # in_test_set = housing_with_id['id'].apply(lambda id_: test_set_check(id_, 0.2, hashlib.md5))
    # # for i in in_test_set:
    #     print(i)
    # print(in_test_set)
    # [-1] < 256*0,2
    # print(disc)
    # print(len_)
    # print(key1)

    housing['income_cat'] = np.ceil(housing['median_income']/1.5)
    housing['income_cat'].where(housing['income_cat'] < 5, 5, inplace=True)
    # print(housing['income_cat'])

    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
    for train_index, test_index in split.split(housing, housing['income_cat']):
        strat_train_set = housing.loc[train_index]
        strat_test_set = housing.loc[test_index]

    print(strat_test_set['income_cat'].value_counts()/len(strat_test_set))
    print(housing['income_cat'].value_counts()/len(housing))

    # for set in(strat_test_set, strat_test_set):
    #     set.drop(['income_cat'], axis=1, inplace=True)


    # Discover and Visualize the Data to Gain Insights
    housing = strat_train_set.copy()
    
    #pandas plots documentation https://pandas.pydata.org/pandas-docs/version/0.15.0/visualization.html
    # housing.plot(kind='scatter', x='longitude', y='latitude')
    # plt.show()

    
    # housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)
    # plt.show()

    from pandas import DataFrame
    randn = np.random.randn
    df = DataFrame(randn(50, 4), columns=['a', 'b', 'c', 'd'])
    print(df.head())

    df.plot(kind='scatter', x='a', y='b')
    plt.show()